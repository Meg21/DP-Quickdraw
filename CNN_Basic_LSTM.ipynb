{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kagge Basic LSTM Using Stroke Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras \n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import save_model\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import ast\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "# from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no 340\n"
     ]
    }
   ],
   "source": [
    "f = open(\"/data/miniclasses.txt\",\"r\")\n",
    "classes = f.readlines()\n",
    "f.close()\n",
    "classes = [c.replace('\\n','').replace(' ','_') for c in classes]\n",
    "n = len(classes)\n",
    "print('no', n)\n",
    "vals = [x for x in range(0,n)]\n",
    "classmap = dict(zip(classes, vals))\n",
    "classmaprev = dict(zip(vals,classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join('shuffledtrain2/', '*.csv'))\n",
    "batchsize =  512\n",
    "img_size = 32\n",
    "N_CLASSES = 340\n",
    "STROKE_COUNT = 196\n",
    "STEPS = 800\n",
    "EPOCHS = 100\n",
    "size = 32\n",
    "TEST_DIR = ''\n",
    "N_FILES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shuffledtrain2/shuffledtrain2_56.csv', 'shuffledtrain2/shuffledtrain2_41.csv', 'shuffledtrain2/shuffledtrain2_76.csv', 'shuffledtrain2/shuffledtrain2_49.csv', 'shuffledtrain2/shuffledtrain2_99.csv', 'shuffledtrain2/shuffledtrain2_18.csv', 'shuffledtrain2/shuffledtrain2_97.csv', 'shuffledtrain2/shuffledtrain2_9.csv', 'shuffledtrain2/shuffledtrain2_79.csv', 'shuffledtrain2/shuffledtrain2_4.csv', 'shuffledtrain2/shuffledtrain2_27.csv', 'shuffledtrain2/shuffledtrain2_8.csv', 'shuffledtrain2/shuffledtrain2_85.csv', 'shuffledtrain2/shuffledtrain2_93.csv', 'shuffledtrain2/shuffledtrain2_13.csv', 'shuffledtrain2/shuffledtrain2_86.csv', 'shuffledtrain2/shuffledtrain2_100.csv', 'shuffledtrain2/shuffledtrain2_91.csv', 'shuffledtrain2/shuffledtrain2_59.csv', 'shuffledtrain2/shuffledtrain2_24.csv', 'shuffledtrain2/shuffledtrain2_94.csv', 'shuffledtrain2/shuffledtrain2_3.csv', 'shuffledtrain2/shuffledtrain2_10.csv', 'shuffledtrain2/shuffledtrain2_62.csv', 'shuffledtrain2/shuffledtrain2_47.csv', 'shuffledtrain2/shuffledtrain2_6.csv', 'shuffledtrain2/shuffledtrain2_70.csv', 'shuffledtrain2/shuffledtrain2_12.csv', 'shuffledtrain2/shuffledtrain2_45.csv', 'shuffledtrain2/shuffledtrain2_39.csv', 'shuffledtrain2/shuffledtrain2_82.csv', 'shuffledtrain2/shuffledtrain2_81.csv', 'shuffledtrain2/shuffledtrain2_43.csv', 'shuffledtrain2/shuffledtrain2_72.csv', 'shuffledtrain2/shuffledtrain2_7.csv', 'shuffledtrain2/shuffledtrain2_90.csv', 'shuffledtrain2/shuffledtrain2_46.csv', 'shuffledtrain2/shuffledtrain2_98.csv', 'shuffledtrain2/shuffledtrain2_17.csv', 'shuffledtrain2/shuffledtrain2_57.csv', 'shuffledtrain2/shuffledtrain2_66.csv', 'shuffledtrain2/shuffledtrain2_33.csv', 'shuffledtrain2/shuffledtrain2_71.csv', 'shuffledtrain2/shuffledtrain2_32.csv', 'shuffledtrain2/shuffledtrain2_25.csv', 'shuffledtrain2/shuffledtrain2_23.csv', 'shuffledtrain2/shuffledtrain2_78.csv', 'shuffledtrain2/shuffledtrain2_53.csv', 'shuffledtrain2/shuffledtrain2_51.csv', 'shuffledtrain2/shuffledtrain2_14.csv', 'shuffledtrain2/shuffledtrain2_21.csv', 'shuffledtrain2/shuffledtrain2_58.csv', 'shuffledtrain2/shuffledtrain2_44.csv', 'shuffledtrain2/shuffledtrain2_38.csv', 'shuffledtrain2/shuffledtrain2_65.csv', 'shuffledtrain2/shuffledtrain2_80.csv', 'shuffledtrain2/shuffledtrain2_30.csv', 'shuffledtrain2/shuffledtrain2_29.csv', 'shuffledtrain2/shuffledtrain2_5.csv', 'shuffledtrain2/shuffledtrain2_88.csv', 'shuffledtrain2/shuffledtrain2_55.csv', 'shuffledtrain2/shuffledtrain2_83.csv', 'shuffledtrain2/shuffledtrain2_84.csv', 'shuffledtrain2/shuffledtrain2_36.csv', 'shuffledtrain2/shuffledtrain2_87.csv', 'shuffledtrain2/shuffledtrain2_74.csv', 'shuffledtrain2/shuffledtrain2_20.csv', 'shuffledtrain2/shuffledtrain2_40.csv', 'shuffledtrain2/shuffledtrain2_89.csv', 'shuffledtrain2/shuffledtrain2_1.csv', 'shuffledtrain2/shuffledtrain2_68.csv', 'shuffledtrain2/shuffledtrain2_37.csv', 'shuffledtrain2/shuffledtrain2_75.csv', 'shuffledtrain2/shuffledtrain2_96.csv', 'shuffledtrain2/shuffledtrain2_31.csv', 'shuffledtrain2/shuffledtrain2_52.csv', 'shuffledtrain2/shuffledtrain2_19.csv', 'shuffledtrain2/shuffledtrain2_16.csv', 'shuffledtrain2/shuffledtrain2_35.csv', 'shuffledtrain2/shuffledtrain2_60.csv', 'shuffledtrain2/shuffledtrain2_54.csv', 'shuffledtrain2/shuffledtrain2_2.csv', 'shuffledtrain2/shuffledtrain2_67.csv', 'shuffledtrain2/shuffledtrain2_77.csv', 'shuffledtrain2/shuffledtrain2_42.csv', 'shuffledtrain2/shuffledtrain2_15.csv', 'shuffledtrain2/shuffledtrain2_61.csv', 'shuffledtrain2/shuffledtrain2_73.csv', 'shuffledtrain2/shuffledtrain2_64.csv', 'shuffledtrain2/shuffledtrain2_95.csv', 'shuffledtrain2/shuffledtrain2_22.csv', 'shuffledtrain2/shuffledtrain2_11.csv', 'shuffledtrain2/shuffledtrain2_92.csv', 'shuffledtrain2/shuffledtrain2_26.csv', 'shuffledtrain2/shuffledtrain2_50.csv', 'shuffledtrain2/shuffledtrain2_63.csv', 'shuffledtrain2/shuffledtrain2_48.csv', 'shuffledtrain2/shuffledtrain2_28.csv', 'shuffledtrain2/shuffledtrain2_69.csv', 'shuffledtrain2/shuffledtrain2_34.csv']\n",
      "[146, 338, 215, 314, 66, 137, 5, 270, 334, 30]\n"
     ]
    }
   ],
   "source": [
    "print(all_files)\n",
    "t = pd.read_csv(all_files[1], nrows = 10)\n",
    "t['drawing'] = t['drawing'].apply(ast.literal_eval)\n",
    "labels = t['word']\n",
    "labels = [c.replace(' ','_') for c in labels]\n",
    "idx = [classmap[x] for x in labels]\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_cv2(raw_strokes, size=256, lw=6, time_color=True):\n",
    "    img = np.zeros((256, 256), np.uint8)\n",
    "    for t, stroke in enumerate(raw_strokes):\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            color = 255 - min(t, 10) * 13 if time_color else 255\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]),\n",
    "                         (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n",
    "    if size != 256:\n",
    "        return cv2.resize(img, (size, size))\n",
    "    else:\n",
    "        return img\n",
    "    \n",
    "\n",
    "def _stack_it(raw_strokes):\n",
    "    \"\"\"preprocess the string and make \n",
    "    a standard Nx3 stroke vector\"\"\"\n",
    "    stroke_vec = ast.literal_eval(raw_strokes) # string->list\n",
    "    # unwrap the list\n",
    "    in_strokes = [(xi,yi,i)  \n",
    "     for i,(x,y) in enumerate(stroke_vec) \n",
    "     for xi,yi in zip(x,y)]\n",
    "    c_strokes = np.stack(in_strokes)\n",
    "    # replace stroke id with 1 for continue, 2 for new\n",
    "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
    "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
    "    # pad the strokes with zeros\n",
    "    return pad_sequences(c_strokes.swapaxes(0, 1), \n",
    "                         maxlen=STROKE_COUNT, \n",
    "                         padding='post').swapaxes(0, 1)\n",
    "    \n",
    "def image_generator_xd(all_files,classmap, size, batchsize,ks,N_CLASSES, lw=6, time_color=True):\n",
    "    while True:\n",
    "        for k in np.random.permutation(ks):\n",
    "            filename = all_files[k]\n",
    "            for df in pd.read_csv(filename, chunksize=batchsize):\n",
    "                y = np.empty([0])\n",
    "                df['drawing'] = df['drawing'].map(_stack_it)\n",
    "                labels = df['word']\n",
    "                labels = [c.replace(' ','_') for c in labels]\n",
    "                idx = [classmap[x] for x in labels]\n",
    "                y = np.append(y, idx)\n",
    "                x = np.stack(df['drawing'], 0)\n",
    "                #x = preprocess_input(x).astype(np.float32)\n",
    "                y = keras.utils.to_categorical(y, N_CLASSES)\n",
    "                yield x, y\n",
    "\n",
    "def df_to_image_array_xd(df, size, lw=6, time_color=True):\n",
    "    df['drawing'] = df['drawing'].apply(ast.literal_eval)\n",
    "    x = np.zeros((len(df), size, size, 1))\n",
    "    for i, raw_strokes in enumerate(df.drawing.values):\n",
    "        x[i, :, :, 0] = draw_cv2(raw_strokes, size=size, lw=lw, time_color=time_color)\n",
    "    x = preprocess_input(x).astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "def get_available_gpus():\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "def preds2catids(predictions):\n",
    "    return pd.DataFrame(np.argsort(-predictions, axis=1)[:, :3], columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_datagen = image_generator_xd(all_files,classmap,size=img_size, N_CLASSES= N_CLASSES,batchsize=batchsize,ks=range(N_FILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 196, 3)\n",
      "(512, 340)\n"
     ]
    }
   ],
   "source": [
    "x_, y= next(train_datagen)\n",
    "print(x_.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, None, 3)           12        \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 48)          768       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 48)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 64)          15424     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 96)          18528     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 96)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, None, 128)         115712    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 128)               132096    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 340)               174420    \n",
      "=================================================================\n",
      "Total params: 523,008\n",
      "Trainable params: 523,002\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout, Bidirectional\n",
    "if len(get_available_gpus())>0:\n",
    "    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\n",
    "stroke_read_model = Sequential()\n",
    "stroke_read_model.add(BatchNormalization(input_shape = (None,)+x_.shape[2:]))\n",
    "# filter count and length are taken from the script https://github.com/tensorflow/models/blob/master/tutorials/rnn/quickdraw/train_model.py\n",
    "stroke_read_model.add(Conv1D(48, (5,)))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Conv1D(64, (5,)))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Conv1D(96, (3,)))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = True))\n",
    "stroke_read_model.add(LSTM(128, return_sequences = False))\n",
    "stroke_read_model.add(Dense(512))\n",
    "stroke_read_model.add(Dropout(0.3))\n",
    "stroke_read_model.add(Dense(340, activation = 'softmax'))\n",
    "stroke_read_model.compile(optimizer = 'adam', \n",
    "                          loss = 'categorical_crossentropy', \n",
    "                          metrics = ['categorical_accuracy', top_3_accuracy])\n",
    "stroke_read_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 95s 118ms/step - loss: 5.6602 - acc: 0.0105 - top_3_accuracy: 0.0287\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 94s 118ms/step - loss: 5.3318 - acc: 0.0217 - top_3_accuracy: 0.0536\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 94s 118ms/step - loss: 5.0162 - acc: 0.0388 - top_3_accuracy: 0.0916\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 95s 119ms/step - loss: 4.6837 - acc: 0.0649 - top_3_accuracy: 0.1463\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 94s 118ms/step - loss: 4.2828 - acc: 0.1067 - top_3_accuracy: 0.2239\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 96s 120ms/step - loss: 3.7497 - acc: 0.1773 - top_3_accuracy: 0.3393\n",
      "Epoch 7/100\n",
      "620/800 [======================>.......] - ETA: 20s - loss: 3.3785 - acc: 0.2404 - top_3_accuracy: 0.4264"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
    "def top_3_accuracy(x,y): \n",
    "    t3 = top_k_categorical_accuracy(x,y, 3)\n",
    "    return t3\n",
    "\n",
    "\n",
    "stroke_read_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', top_3_accuracy])\n",
    "\n",
    "hists = []\n",
    "hist = stroke_read_model.fit_generator(\n",
    "    train_datagen, steps_per_epoch=STEPS, epochs=EPOCHS, verbose=1,\n",
    ")\n",
    "hists.append(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(test_path)\n",
    "sub_df['drawing'] = sub_df['drawing'].map(_stack_it)\n",
    "sub_vec = np.stack(sub_df['drawing'].values, 0)\n",
    "sub_pred = stroke_read_model.predict(sub_vec, verbose=True, batch_size=batch_size)\n",
    "top_3_pred = [word_encoder.classes_[np.argsort(-1*c_pred)[:3]] for c_pred in sub_pred]\n",
    "top_3_pred = [' '.join([col.replace(' ', '_') for col in row]) for row in top_3_pred]\n",
    "sub_df['word'] = top_3_pred\n",
    "sub_df[['key_id', 'word']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
